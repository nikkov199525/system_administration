# Отчет
# По лабораторной  работе № 2 
Студент: Ковалев Никита Александрович
Преподаватель: Некрасов Евгений Андреевич 
Группа: p4250  
Дата выполнения: 03.11.2025  
дисциплина: Системное администрирование
### Задание 
# Построение L2-сети и настройка внутреннего DNS

## Цель работы

Научиться:

* объединять несколько серверов в одну логическую L2-сеть поверх существующей IP-сети;
* использовать VPN-технологии (fastd, tinc, VXLAN) для создания оверлейных сетей;
* настраивать внутренний DNS для удобного доступа к серверам по доменным именам в выделенной зоне.


Вам предоставлено **несколько виртуальных серверов** (не менее трёх). Между ними есть только стандартная L3-сеть (например, приватные IP в облаке или NAT-сеть в VirtualBox).

Ваша задача —:

1. Построить **виртуальную L2-сеть** поверх этой инфраструктуры.
   Все серверы должны оказаться в одной подсети, как если бы они были подключены к одному физическому свитчу.
2. Развернуть на одном из узлов **DNS-сервер**, который будет обслуживать внутреннюю зону `dc.local`.
   В этой зоне должны автоматически или вручную появляться имена всех серверов (`<hostname>.dc.local`).
3. Настроить резолвинг так, чтобы с любого узла можно было обратиться к другому по имени вида `nginx01.dc.local`.

---

## Часть 1. Построение L2-сети

1. Выберите один из вариантов:

   * **tinc** — децентрализованный VPN, поддерживает mesh-топологии;
   * **VXLAN** — оверлейный протокол для построения L2-сетей поверх L3.

   *(Можно реализовать несколько способов и сравнить, но достаточно одного.)*

2. Реализуйте схему:

   * **  fastd** — простой и быстрый VPN для L2/L3 поверх UDP;
   * Настройте VPN/overlay на всех серверах.
   * Создайте интерфейсы (например, `vxlan100` или `tinc0`).
   * Сведите интерфейсы в bridge (`br0`), чтобы они стали частью одной L2-сети.
   * Назначьте каждому серверу адрес из общей подсети, например `10.100.0.0/24`.

3. Убедитесь, что:

   * все сервера видят друг друга по ARP (`arp -a` должен показывать MAC-адреса соседей);
   * `ping` по внутренним IP работает между всеми узлами;
   * сеть действительно L2, а не просто набор туннелей.


## Часть 2. DNS-сервер

1. Выберите DNS-сервер:

   * **bind9** (классический DNS-сервер, поддержка зон, динамических обновлений);
   * **dnsmasq** (лёгкий сервер, совмещает DHCP и DNS, удобен для небольших сетей).

2. Настройте зону `dc.local`:

   * Каждому серверу выдайте имя вида `<hostname>.dc.local`.
   * Примеры: `nginx01.dc.local`, `db01.dc.local`, `studentpc.dc.local`.
   * В зоне должны быть A-записи с IP из вашей новой L2-сети.

3. Настройте все узлы так, чтобы они использовали этот DNS-сервер.

   * Проверьте, что команды `dig` или `host` возвращают корректные IP.
   * Проверьте, что можно `ping` по доменному имени.

---

## Часть 3. Артефакты и оформление

1. В отдельном репозитории разместите:

   * конфигурационные файлы (`fastd.conf`, `tinc.conf`, `systemd-networkd`-файлы, `bind9`-зоны и т. д.);
   * скрипты для запуска/остановки сервисов;
   * документацию в `README.md`.

2. В `README.md` должно быть:

   * **Краткое описание топологии** (какие узлы, какие IP, какой софт используется);
   * **Пошаговое руководство**, как развернуть сеть и DNS с нуля;
   * **Результаты проверки** в текстовом виде:

     * вывод `ip addr show` с новыми интерфейсами;
     * вывод `bridge link` или `ip link show type vxlan`;
     * вывод `ping` между узлами;
     * вывод `dig nginx01.dc.local`.

3. **Важно:** никаких скриншотов. Все результаты должны быть в текстовом виде, либо как лог выполнения команд, либо как отдельные `.txt` файлы в репозитории.

---

## Подготовка

Все выполнялось на реальных машинах, которым здесь присвоены названия server01, server02, server03.
Подразумевается, что server01 принимает соединения. Он должен иметь публичный IP адрес, или возможность проброса портов. Серверы 2 и 3 могут находится за NATами. Они будут устанавливать соединения с сервером 1.
В текущей сети server01 имеет адрес 192.168.0.8
В виртуальной сети tinc серверы будут иметь адреса 10.100.0.1, 10.100.0.2, 10.100.0.3

## Часть 1. Построение L2-сети
1.1 Данные действия производим на всех машинах.

Устанавливаем tinc: sudo apt install tinc

Создаем структуру каталогов конфигурации для VPN: sudo mkdir -p /etc/tinc/local/hosts
где local - имя сети

Создаем файлы конфигурации.
Tinc требует, чтобы каждая машина, которая будет частью VPN, имела следующие три компонента конфигурации:
Файлы конфигурации Tinc : существует три отдельных файла, которые настраивают демон Tinc:

tinc.conf, который определяет сетевое имя, сетевое устройство, на котором будет работать VPN, и другие параметры VPN. В этом файле задается режим работы tinc: router, switch, или hub. На серверах 2 и 3 этот файл содержит ConnectTo директиву, указывающую на server_01, тогда как файл сервера server-01tinc.conf не включает эту директиву. Отсутствие ConnectTo инструкции для сервера server-01 означает, что сервер server-01 будет прослушивать только входящие соединения. Это подходит для нашей конфигурации, поскольку он не будет подключаться к другим машинам.
Name = server_01
AddressFamily = ipv4
DeviceType = tap
Mode = hub
Interface = tap1

tinc-up, скрипт, который активирует сетевое устройство, определенное в tinc.conf после запуска tinc;
#!/bin/sh
ip link set $INTERFACE up
ip addr add 10.0.0.1/24 dev $INTERFACE
ip route add 10.0.0.0/24 dev $INTERFACE


tinc-down, который деактивирует сетевое устройство при каждой остановке tinc.
#!/bin/sh
ip route del 10.0.0.0/24 dev $INTERFACE
ip addr del 10.0.0.1/24 dev $INTERFACE
ip link set $INTERFACE down

Пары открытого и закрытого ключей : Tinc использует пары открытого и закрытого ключей, чтобы гарантировать, что доступ к VPN смогут получить только пользователи с действительными ключами.
Генерируем ключи: sudo tincd -n netname -K4096
Файлы конфигурации хоста : каждая машина имеет свой собственный файл конфигурации, содержащий фактический IP-адрес хоста(только для server01) и публичный ключ. Копируем эти файлы по сети, они должны быть идентичны на всех машинах.

Необходимо сделать оба скрипта исполняемыми: sudo chmod 755 /etc/tinc/netname/tinc-*

Добавляем правило брандмауэра sudo ufw allow 655

1.2 Тестирование
На каждом узле, начиная с server-01 , запускаем tinc с помощью следующей команды:
sudo tincd -n netname -D -d3
Проверяем ping и ip neighbor на всех машинах.
В режиме работы hub видим одинаковые mac адреса на сетевом интерфесе tinc на всех трех машинах.

1.3 Настройка Tinc для запуска при загрузке системы.
Выполните следующую команду на каждом узле , чтобы настроить запуск tinc VPN при каждой загрузке машин: sudo systemctl enable tinc@netname.
Tinc настроен на запуск при загрузке каждой из ваших машин, и вы можете управлять им с помощью systemctl команды. Если вы хотите запустить его сейчас, выполните следующую команду на каждом из ваших узлов: sudo systemctl start tinc@local
Для остановки: sudo systemctl stop tinc@local

## Часть 2. DNS-сервер
Устанавливаем на server01 Dnsmasq: sudo apt-get install dnsmasq

Устанавливаем resolvconf: sudo apt-get install resolvconf

Отредактируйте файл /etc/dnsmasq.conf:

sudo nano /etc/dnsmasq.conf

По умолчанию все настройки в этом файле закомментированы. Если какие-то настройки в этом файле у вас заданы, то закомментируйте их обратно и оставьте только те, которые перечислены ниже. Добавлять и менять настройки рекомендуется после проверки исправной работы DNS-сервера.

no-resolv

Эта настройка выключает загрузку настроек из /etc/resolv.conf. Все настройки будут браться из редактируемого файла /etc/dnsmasq.conf . Это сильно упрощает конфигурацию Dnsmasq'а, поскольку файл /etc/resolv.conf автоматически пересоздаётся при рестарте системы.

server=8.8.8.8 

8.8.8.8 - это адрес DNS-сервера Гугл. Этот адрес можно заменить на любой другой адрес публичного DNS-сервера. Например, на адрес DNS-сервера вашего провайдера или ранее используемого DNS-сервера.

Запросы, которые не сможет обработать Dnsmasq будут направлены на этот сервер.

listen-address=0.0.0.0

Эта настройка позволит осуществлять запросы к Dnsmasq'у с других хостов.

bind-interfaces

Задаёт режим, при котором Dnsmasq не осуществляет привязку к интерфейсам, по которым не должна осуществляться обработка запросов. Без этой настройки в предлагаемом варианте конфигурации сервер не работает.

Добавьте в файл /etc/hosts необходимые домены и их IP адреса.

sudo nano /etc/hosts

10.100.0.1  server01.dc.local
10.100.0.2  server02.dc.local
10.100.0.3  server03.dc.local

Перезагрузить машину.
Создать правило для брандмауэра ufw allow 53

На машинах 2 и 3 настроить данный DNS в настройках сетевых интерфейсов.

## Результаты проверки. Вывод на server02

server02:~$ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host noprefixroute 
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:84:6b:d6 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.7/24 brd 192.168.1.255 scope global dynamic noprefixroute enp0s3
       valid_lft 86350sec preferred_lft 86350sec
3: tap1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UNKNOWN group default qlen 1000
    link/ether ee:3d:fd:7f:0e:5f brd ff:ff:ff:ff:ff:ff
    inet 10.100.0.2/24 scope global tap1
       valid_lft forever preferred_lft forever
    inet6 fe80::ec3d:fdff:fe7f:e5f/64 scope link 
       valid_lft forever preferred_lft forever

server02:~$ ping 10.100.0.1
PING 10.100.0.1 (10.100.0.1) 56(84) bytes of data.
64 bytes from 10.100.0.1: icmp_seq=1 ttl=64 time=0.795 ms
64 bytes from 10.100.0.1: icmp_seq=2 ttl=64 time=0.995 ms
64 bytes from 10.100.0.1: icmp_seq=3 ttl=64 time=1.14 ms

server02:~$ ping 10.100.0.3
PING 10.100.0.3 (10.100.0.3) 56(84) bytes of data.
64 bytes from 10.100.0.3: icmp_seq=1 ttl=64 time=2.31 ms
64 bytes from 10.100.0.3: icmp_seq=2 ttl=64 time=1.41 ms
64 bytes from 10.100.0.3: icmp_seq=3 ttl=64 time=1.33 ms

erver02:~$ ping server01.dc.local
PING server01.dc.local (10.100.0.1) 56(84) bytes of data.
64 bytes from server01.dc.local (10.100.0.1): icmp_seq=1 ttl=64 time=0.828 ms
64 bytes from server01.dc.local (10.100.0.1): icmp_seq=2 ttl=64 time=1.01 ms
64 bytes from server01.dc.local (10.100.0.1): icmp_seq=3 ttl=64 time=0.826 ms

ping server03.dc.local
PING server03.dc.local (10.100.0.3) 56(84) bytes of data.
64 bytes from server03.dc.local (10.100.0.3): icmp_seq=1 ttl=64 time=0.880 ms
64 bytes from server03.dc.local (10.100.0.3): icmp_seq=2 ttl=64 time=1.52 ms
64 bytes from server03.dc.local (10.100.0.3): icmp_seq=3 ttl=64 time=1.27 ms

server02:~$ dig server01.dc.local

; <<>> DiG 9.18.33-1~deb12u2-Debian <<>> server01.dc.local
;; global options: +cmd
;; Got answer:
;; WARNING: .local is reserved for Multicast DNS
;; You are currently testing what happens when an mDNS query is leaked to DNS
;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 19676
;; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1

;; OPT PSEUDOSECTION:
; EDNS: version: 0, flags:; udp: 1232
;; QUESTION SECTION:
;server01.dc.local.		IN	A

;; ANSWER SECTION:
server01.dc.local.	0	IN	A	10.100.0.1

;; Query time: 4 msec
;; SERVER: 10.100.0.1#53(10.100.0.1) (UDP)
;; WHEN: Mon Oct 13 20:18:16 EDT 2025
;; MSG SIZE  rcvd: 62


